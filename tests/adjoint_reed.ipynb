{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c04e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import basix\n",
    "import matplotlib.pyplot as plt\n",
    "from basix import CellType, ElementFamily, LagrangeVariant, LatticeType\n",
    "import jax_pn\n",
    "from jax.experimental import sparse\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import jax\n",
    "#jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a87845",
   "metadata": {},
   "source": [
    "# Adjoint method with Autodiff partial derivatives\n",
    "\n",
    "We define the adjoint variable $\\boldsymbol{\\lambda} \\in \\mathbb{R}^N$ as the solution to:\n",
    "$$\n",
    "    \\left( \\frac{\\partial \\boldsymbol{R}}{\\partial \\boldsymbol{x}} \\right)^T \\boldsymbol{\\lambda} = \\left( \\frac{\\partial f}{\\partial \\boldsymbol{x}} \\right)^T\n",
    "$$\n",
    "\n",
    "\n",
    "This allows us to write the objective sensitivity as:\n",
    "$$\n",
    "    \\frac{d f}{d \\boldsymbol{\\theta}} \n",
    "    = -\\boldsymbol{\\lambda}^T \\frac{\\partial \\boldsymbol{R}}{\\partial \\boldsymbol{\\theta}} + \\frac{\\partial f}{\\partial \\boldsymbol{\\theta}}\n",
    "$$\n",
    "\n",
    "In this notebook, the adjoint method is implemented and compared to the finite difference method using the autodiff for the partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "lagrange    = basix.create_element(ElementFamily.P, CellType.interval, degree= 5, lagrange_variant= LagrangeVariant.gll_warped)\n",
    "N_max = 3\n",
    "elements_per_cm = 10\n",
    "regions = [\n",
    "    (2.0, [20.0], np.array([[[0.0]]]), [20.0]),\n",
    "    (1.0, [1.0],  np.array([[[0.0]]]),  [0.0]),\n",
    "    (2.0, [0.0],  np.array([[[0.0]]]),  [0.0]),\n",
    "    (1.0, [1.0],  np.array([[[0.9]]]), [1.0]),\n",
    "    (2.0, [1.0],  np.array([[[0.9]]]), [0.0]),\n",
    "]\n",
    "\n",
    "adpn_prob = jax_pn.ADPN.ADPN_Problem.from_regions_per_cm(regions, elements_per_cm, N_max, lagrange, L_scat=0)\n",
    "solution  = adpn_prob.Solve_Multigroup_System('vacuum',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b9d47",
   "metadata": {},
   "source": [
    "#### Checking the residual\n",
    "\n",
    "To check whether the residual was correctly implemented, we check it by comparing it with the matrix construction.\n",
    "\n",
    "We also check that $\\partial R / \\partial x$ reproduces the original finite-element matrix (including the augmented boundary conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281709bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_pn.ADPN import residualPN_jit, GlobalSettings\n",
    "parameters_eg = {\n",
    "    'sigma_t_i'       : adpn_prob.jax_sigma_t,\n",
    "    'sigma_s_k_i_gg'  : adpn_prob.jax_sigma_s,\n",
    "    'h_i'             : adpn_prob.jax_h_i,\n",
    "    'q_i_k_j'         : adpn_prob.jax_q_i_k_j\n",
    "}\n",
    "\n",
    "def residual_naive(global_settings : GlobalSettings, matrix_settings, parameters, solution):\n",
    "    data, rows, cols, bdata, brows = jax_pn.ADPN.total_matrix_assembly_vacuum_bcs_single_g_jit(global_settings, matrix_settings, parameters)\n",
    "    total_dofs = global_settings.n_dofs_per_eg    \n",
    "    \n",
    "    indices = jnp.stack([rows, cols], axis=1)\n",
    "    A_total = jax.experimental.sparse.BCOO((data, indices), shape=(total_dofs, total_dofs))\n",
    "    b_cols = jnp.zeros_like(brows)\n",
    "    b_indices = jnp.stack([brows, b_cols], axis=1)    \n",
    "    b_total = jax.experimental.sparse.BCOO((bdata, b_indices), shape=(total_dofs, 1))        \n",
    "\n",
    "    return (A_total @ solution - b_total.todense()[:,0])\n",
    "\n",
    "np.set_printoptions(precision=12)\n",
    "res_old = residualPN_jit(adpn_prob.global_settings, adpn_prob.matrix_settings, parameters_eg, solution)\n",
    "res_new = residual_naive(adpn_prob.global_settings, adpn_prob.matrix_settings, parameters_eg, solution)\n",
    "print(\"Difference: \", np.max(np.abs(res_old - res_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc185b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_total_f(global_settings : GlobalSettings, matrix_settings, parameters, solution):\n",
    "    data, rows, cols, bdata, brows = jax_pn.ADPN.total_matrix_assembly_vacuum_bcs_single_g_jit(global_settings, matrix_settings, parameters)\n",
    "    total_dofs = global_settings.n_dofs_per_eg\n",
    "        \n",
    "    indices = jnp.stack([rows, cols], axis=1)\n",
    "    A_total = jax.experimental.sparse.BCOO((data, indices), shape=(total_dofs, total_dofs))\n",
    "    return A_total\n",
    "jac_A    = jax.jit(jax.jacfwd(residualPN_jit, argnums = (3,)), static_argnums = 0)\n",
    "a_normal = A_total_f(adpn_prob.global_settings, adpn_prob.matrix_settings, parameters_eg, solution).todense()\n",
    "Ares     = jac_A(adpn_prob.global_settings, adpn_prob.matrix_settings, parameters_eg, solution)[0]\n",
    "print(np.max(np.abs(a_normal - Ares)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372dfa8",
   "metadata": {},
   "source": [
    "### Defining an objective function\n",
    "\n",
    "We define a very simple objective function here: the flux at the edge of the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_flux(sol, settings : GlobalSettings):\n",
    "   right_dof = settings.right_dof\n",
    "   return sol[right_dof]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de0510",
   "metadata": {},
   "source": [
    "This solves the adjoint system:\n",
    "\n",
    "- First, $\\partial f / \\partial_x $ is computed using autodiff (this is a very simple differentation, but we keep it as-is).\n",
    "- The $\\partial R / \\partial_x $ is not computed using autodiff but just by constructing the sparse matrix.\n",
    "\n",
    "Then, using scipy, the adjoint system is solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdx = jax.grad(edge_flux, argnums = 0)(solution, adpn_prob.global_settings)\n",
    "\n",
    "at_jax = A_total_f(adpn_prob.global_settings, adpn_prob.matrix_settings, parameters_eg, solution).T\n",
    "\n",
    "data = np.array(at_jax.data)\n",
    "indices = np.array(at_jax.indices)\n",
    "shape = at_jax.shape\n",
    "import scipy \n",
    "import scipy.sparse as sp\n",
    "at_scipy = scipy.sparse.coo_matrix((data, indices.T), shape=shape).tocsr()\n",
    "adjoint_var = sp.linalg.spsolve(at_scipy, np.array(dfdx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3cd20",
   "metadata": {},
   "source": [
    "Computing the $\\partial R / \\partial \\theta $ term is then rather simple using autodiff. Note that parameters is actually a dictionary, but JAx handles this as well, returning a dictionary with the partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "drdtheta = jax.jit(jax.jacfwd(residualPN_jit, argnums = 2), static_argnums=0)\n",
    "drdtheta_res = drdtheta(adpn_prob.global_settings, adpn_prob.matrix_settings, parameters_eg, solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameters_eg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684eab08",
   "metadata": {},
   "source": [
    "Finally, the adjoint is used to obtain the desired total derivatives.\n",
    "\n",
    "Here, we just use the derivative of the fast flux w.r.t to the element sizes (note it gets them all at the same time!) and the scattering matrices (which are 320 design variables!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52da040",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdh = np.einsum(\"i, ik->k\", adjoint_var, - np.array(drdtheta_res['h_i']))\n",
    "dfdsigma_s = np.einsum(\"i, iklgp->klgp\", adjoint_var, -np.array(drdtheta_res['sigma_s_k_i_gg']))\n",
    "dfdq= np.einsum(\"i, ikljg->kljg\", adjoint_var, -np.array(drdtheta_res['q_i_k_j']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0434c8",
   "metadata": {},
   "source": [
    "A comparison with a naive finite-difference approach shows that the adjoint method provides the correct derivatives. However, note that the finite-difference method would require $80 + 320 + 80 + 480 \\times 4 = 2400 $ function evaluations to get \n",
    "the same number of derivatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_with_param(parameters_eg):\n",
    "        a = jax_pn.ADPN.total_matrix_assembly_vacuum_bcs_single_g_jit(adpn_prob.global_settings, adpn_prob.matrix_settings, parameters_eg)\n",
    "        A_total = scipy.sparse.coo_matrix((a[0], (a[1], a[2])), shape=(adpn_prob.global_settings.n_dofs_per_eg, adpn_prob.global_settings.n_dofs_per_eg)).tocsr()\n",
    "        b_total = scipy.sparse.coo_matrix((a[3], (a[4], np.zeros_like(a[4]))), shape=(adpn_prob.global_settings.n_dofs_per_eg, 1)).tocsr()\n",
    "        u_base  = sp.linalg.spsolve(A_total, b_total.todense()[:,0])\n",
    "        return edge_flux(u_base, adpn_prob.global_settings)\n",
    "    \n",
    "def finite_difference_h(index, stepsize = 1e-4):\n",
    "    h_i_default = np.array(adpn_prob.jax_h_i)\n",
    "\n",
    "\n",
    "    parameters_eg = {\n",
    "        'sigma_t_i'       : adpn_prob.jax_sigma_t[:, 0],\n",
    "        'sigma_s_k_i_gg'  : adpn_prob.jax_sigma_s[:, :, 0, 0],\n",
    "        'h_i'             : h_i_default,\n",
    "        'q_i_k_j'         : adpn_prob.jax_q_i_k_j[:, :, :, 0]\n",
    "    }\n",
    "    base = compute_with_param(parameters_eg)\n",
    "    h_i_perturbed = np.copy(h_i_default)\n",
    "    h_i_perturbed[index] += stepsize\n",
    "    parameters_eg['h_i'] = h_i_perturbed    \n",
    "    edge = compute_with_param(parameters_eg)\n",
    "    return (edge - base) / stepsize\n",
    "    \n",
    "index = -5\n",
    "print(f\"H comparison: index {index}\")\n",
    "print(\"Autodiff\",  dfdh[index])\n",
    "print(\"Finite Diff\", finite_difference_h(index))\n",
    "\n",
    "\n",
    "def finite_difference_sigma_s(index,l_value,  stepsize = 1e-4):\n",
    "    \n",
    "    sigma_s_default = adpn_prob.jax_sigma_s[:, :, 0, 0]\n",
    "    parameters_eg = {\n",
    "        'sigma_t_i'       : adpn_prob.jax_sigma_t[:, 0],\n",
    "        'sigma_s_k_i_gg'  : sigma_s_default,\n",
    "        'h_i'             : adpn_prob.jax_h_i,\n",
    "        'q_i_k_j'         : adpn_prob.jax_q_i_k_j[:, :, :, 0]\n",
    "    }\n",
    "\n",
    "    base = compute_with_param(parameters_eg)\n",
    "\n",
    "    sigma_s_perturbed = np.copy(sigma_s_default)\n",
    "    sigma_s_perturbed[index, l_value] += stepsize #* sigma_s_perturbed[index, l_value] \n",
    "    \n",
    "    parameters_eg['sigma_s_k_i_gg'] = sigma_s_perturbed\n",
    "    edge = compute_with_param(parameters_eg)\n",
    "    return (edge - base) / stepsize\n",
    "    \n",
    "index = - 10 \n",
    "l_value = 1\n",
    "print(f\"Sigma_s comparison: index {index}, l_value {l_value}\")\n",
    "print(\"Autodiff:\" , dfdsigma_s[index, l_value])\n",
    "print(\"Finite diff\", finite_difference_sigma_s(index, l_value, stepsize=1e-2))\n",
    "\n",
    "def finite_difference_q(index,l_value, local_dof,  stepsize = 1e-4):\n",
    "    \n",
    "    q_default = adpn_prob.jax_q_i_k_j[:, :, :, 0]\n",
    "    parameters_eg = {\n",
    "        'sigma_t_i'       : adpn_prob.jax_sigma_t[:, 0],\n",
    "        'sigma_s_k_i_gg'  : adpn_prob.jax_sigma_s[:, :, 0, 0],\n",
    "        'h_i'             : adpn_prob.jax_h_i,\n",
    "        'q_i_k_j'         : q_default\n",
    "    }\n",
    "\n",
    "    base = compute_with_param(parameters_eg)\n",
    "\n",
    "    sigma_s_perturbed = np.copy(q_default)\n",
    "    sigma_s_perturbed[index, l_value, local_dof] += stepsize #* sigma_s_perturbed[index, l_value] \n",
    "    \n",
    "    parameters_eg['q_i_k_j'] = sigma_s_perturbed\n",
    "    edge = compute_with_param(parameters_eg)\n",
    "    return (edge - base) / stepsize\n",
    "    \n",
    "index = - 1 \n",
    "l_value = 1\n",
    "local_dof = 0\n",
    "print(f\"Q comparison: index {index}, l_value {l_value}, local_dof {local_dof}\")\n",
    "print(\"Autodiff:\" , dfdq[index, l_value, local_dof])\n",
    "print(\"Finite diff\", finite_difference_q(index, l_value, local_dof, stepsize=1e-2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
